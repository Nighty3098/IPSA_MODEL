# Stock Price Prediction Model Documentation ğŸ“ˆ

**Version 2.0** | **Last Updated: March 01, 2026**

---

## ğŸ“– Overview

This project implements an advanced deep learning model for predicting stock prices using historical market data. The model leverages a hybrid architecture that combines **causal convolutional layers**, **multiâ€‘head selfâ€‘attention**, **residual connections**, and **layer normalization** to effectively capture both shortâ€‘term patterns and longâ€‘term dependencies in financial time series. The model is trained on multiple stock tickers and predicts the future closing price based on a window of past observations.

The codebase is written in Python, using **TensorFlow/Keras** for model construction, **scikitâ€‘learn** for preprocessing, and **pandas** for data manipulation. The pipeline includes robust data scaling, sequence generation, training with advanced callbacks, and thorough evaluation (MAE, MSE, RÂ², and optionally MAPE).

---

## ğŸš€ Features

- **Data Preprocessing**: Loads and cleans stock data for multiple tickers; applies perâ€‘ticker MinMax scaling; handles missing values via forward/backward fill.
- **Sequence Creation**: Builds timeâ€‘series sequences with a configurable window size (default 60 days).
- **Stateâ€‘ofâ€‘theâ€‘Art Architecture**:
  - Causal Conv1D layers with residual connections and layer normalization.
  - Multiâ€‘Head Selfâ€‘Attention for capturing global dependencies.
  - Feedâ€‘forward networks with dropout and L2 regularization.
  - Global average pooling followed by dense heads.
- **Training Pipeline**: Automatic device selection (CPU/GPU); callbacks for early stopping, model checkpointing, learning rate reduction, and TensorBoard logging.
- **Evaluation**: Computes MAE, MSE, and RÂ² on the test set; plots training curves.
- **Model Persistence**: Saves the final model and perâ€‘ticker scalers for later inference.
- **Visualization**: Generates training/validation loss and metric plots.

---

## ğŸ› ï¸ Requirements

Install the required packages:

```bash
pip install pandas numpy tensorflow scikit-learn joblib matplotlib
```

Or use a `requirements.txt` in InvestingAssistant repo:

```text
pandas>=2.0.0
numpy>=1.24.0
tensorflow>=2.12.0
scikit-learn>=1.2.0
joblib>=1.2.0
matplotlib>=3.7.0
```

---

## ğŸ“‚ Project Structure

```
price/
â”œâ”€â”€ combined_stock_data.csv      # Input dataset (userâ€‘provided)
â”œâ”€â”€ stock_model.keras             # Final trained model
â”œâ”€â”€ best_model.keras              # Best checkpoint (by val_loss)
â”œâ”€â”€ stock_scaler.save              # Saved MinMaxScaler per ticker
â”œâ”€â”€ training_log.csv               # Epochâ€‘wise training metrics
â”œâ”€â”€ training_metrics.png           # Plot of loss & metrics
â”œâ”€â”€ logs/                          # TensorBoard logs
â””â”€â”€ train.py                        # Main training script
```

---

## ğŸ“Š Data Format

The input CSV must contain the following columns:

| Column        | Description                          | Type       |
|---------------|--------------------------------------|------------|
| Date          | Date of the observation              | datetime   |
| Ticker        | Stock ticker symbol                  | string     |
| Open          | Opening price                        | float      |
| High          | Highest price of the day             | float      |
| Low           | Lowest price of the day              | float      |
| Close         | Closing price (prediction target)    | float      |
| Volume        | Trading volume                       | float      |
| Dividends     | Dividends paid                       | float      |
| Stock Splits  | Stock split ratio                    | float      |

**Example:**

```csv
Date,Ticker,Open,High,Low,Close,Volume,Dividends,Stock Splits
2023-01-01,AAPL,130.28,132.67,129.61,131.86,123456789,0.0,0.0
2023-01-01,MSFT,240.22,243.15,238.75,241.01,987654321,0.0,0.0
...
```

---

## âš™ï¸ Configuration

The main script defines several constants at the top of `main.py`:

| Parameter       | Description                              | Default Value |
|-----------------|------------------------------------------|---------------|
| `WINDOW_SIZE`   | Number of past days used for prediction  | 60            |
| `EPOCHS`        | Maximum number of training epochs        | 1000          |
| `BATCH_SIZE`    | Batch size for training                  | 128           |

These can be adjusted directly in the source file.

---

## ğŸƒâ€â™‚ï¸ Running the Project

1. **Place your dataset** as `combined_stock_data.csv` in the project directory.
2. **Run the script**:

   ```bash
   python train.py
   ```

   You will be prompted to choose the device:
   ```
   Choose device for training (cpu/gpu):
   ```

3. **Outputs**:
   - `stock_model.keras` â€“ the final trained model.
   - `best_model.keras` â€“ the best model based on validation loss.
   - `stock_scaler.save` â€“ a dictionary of `MinMaxScaler` objects for each ticker.
   - `training_log.csv` â€“ CSV with perâ€‘epoch metrics.
   - `training_metrics.png` â€“ plot of loss, MAE, and MSE.
   - `logs/` â€“ TensorBoard logs.

4. **Monitor with TensorBoard**:

   ```bash
   tensorboard --logdir logs/
   ```

   Then open `http://localhost:6006` in your browser.

---

## ğŸ§  Model Architecture (Improved Version 2.0)

The model is a custom deep architecture designed for timeâ€‘series forecasting. Below is a layerâ€‘byâ€‘layer description:

### 1. Input and Noise Regularisation
- **Input shape**: `(WINDOW_SIZE, num_features)` (e.g., `(60, 7)`).
- **GaussianNoise(0.01)** â€“ adds small noise to inputs for better generalisation.

### 2. Convolutional Blocks with Residual Connections
Three convolutional blocks, each consisting of:
- **Causal Conv1D** (filters: 64, 128, 256; kernel sizes: 7, 5, 5; padding='causal').
- **LayerNormalization** â€“ normalises across the feature dimension (preferred for sequences).
- **Dropout(0.2)** for regularisation.
- **Residual addition**: if the number of filters changes, a 1x1 convolution projects the skip connection.
- **Activation**: ReLU.

### 3. MaxPooling
- **MaxPooling1D(pool_size=2)** â€“ reduces temporal dimension after convolutions.

### 4. Multiâ€‘Head Selfâ€‘Attention Block
- **MultiHeadAttention(num_heads=4, key_dim=128)** â€“ attends to the sequence to capture global dependencies.
- **Residual connection** around the attention layer.
- **LayerNormalisation** after addition.
- **Feedâ€‘forward network**: Dense(ff_dim*2) â†’ Dense(original_dim) with ReLU and Dropout(0.3).
- Another residual connection + layer norm.

### 5. Global Pooling
- **GlobalAveragePooling1D** â€“ aggregates the sequence into a fixedâ€‘length vector.

### 6. Dense Head
- **Dense(256, activation='relu', L2=0.001)** â†’ BatchNormalisation â†’ Dropout(0.4)
- **Dense(128, activation='relu', L2=0.001)** â†’ BatchNormalisation â†’ Dropout(0.3)
- **Output Dense(1)** â€“ predicts the scaled closing price.

### 7. Compilation
- **Optimizer**: AdamW (learning rate = 1e-3, weight decay = 1e-4)
- **Loss**: Mean Squared Error (MSE)
- **Metrics**: MAE, MSE, and MAPE (note: MAPE can be extremely high on scaled data; interpret with caution).

---

## ğŸ“ˆ Training and Evaluation

### Callbacks
- `ModelCheckpoint` â€“ saves the best model (`best_model.keras`) based on `val_loss`.
- `EarlyStopping` â€“ stops after 15 epochs without improvement, restores best weights.
- `ReduceLROnPlateau` â€“ reduces learning rate by factor 0.5 if `val_loss` plateaus for 5 epochs.
- `TensorBoard` â€“ logs to `./logs/`.
- `CSVLogger` â€“ writes epoch metrics to `training_log.csv`.

### Evaluation Metrics on Test Set
After training, the script reports:
- **Test Loss** (MSE)
- **Test MAE**
- **Test MSE** (redundant, kept for clarity)
- **RÂ² Score** (coefficient of determination)

**Typical results** (example from a recent run):
```
Test Loss: 0.0023
Test MAE: 0.0394
Test MSE: 0.0020
Test Accuracy (RÂ² score): 96.74%
```

*Note:* The RÂ² score of 96.74% indicates excellent fit on the test data.

---

## ğŸ”® Making Predictions with the Trained Model

After training, you can load the model and scalers to predict future prices for a specific ticker:

```python
import pandas as pd
import numpy as np
import joblib
from tensorflow.keras.models import load_model

# Load model and scalers
model = load_model("stock_model.keras")
scalers = joblib.load("stock_scaler.save")

# Prepare data for a single ticker (e.g., "AAPL")
ticker = "AAPL"
df = pd.read_csv("combined_stock_data.csv")
company_df = df[df["Ticker"] == ticker].copy()
company_df = company_df.drop(columns=["Date", "Ticker"])

numeric_cols = ["Open", "High", "Low", "Close", "Volume", "Dividends", "Stock Splits"]
company_df = company_df[numeric_cols].ffill().bfill()

# Scale the data using the ticker's scaler
scaler = scalers[ticker]
scaled_data = scaler.transform(company_df[numeric_cols])
scaled_df = pd.DataFrame(scaled_data, columns=numeric_cols)

# Create the last window of length WINDOW_SIZE
if len(scaled_df) < WINDOW_SIZE:
    raise ValueError("Not enough data to form a sequence")
sequence = scaled_df.iloc[-WINDOW_SIZE:].values  # shape: (WINDOW_SIZE, num_features)

# Add batch dimension
sequence = np.expand_dims(sequence, axis=0)  # shape: (1, WINDOW_SIZE, num_features)

# Predict
pred_scaled = model.predict(sequence)[0, 0]

# Inverse transform to get actual price
# Create a dummy row to invert only the 'Close' column
dummy = np.zeros((1, len(numeric_cols)))
dummy[0, numeric_cols.index("Close")] = pred_scaled
pred_actual = scaler.inverse_transform(dummy)[0, numeric_cols.index("Close")]

print(f"Predicted closing price for {ticker}: ${pred_actual:.2f}")
```

---

## ğŸ“ Notes and Caveats

- **MAPE on scaled data**: The Mean Absolute Percentage Error can become astronomically large when the true value is close to zero (because of the division by a small number). **Ignore MAPE** during training if you normalise your targets to [0,1]. For business interpretation, compute MAPE after inverseâ€‘transforming predictions.
- **Data quality**: The model assumes clean, complete data. Forward/backward fill is used, but extreme outliers may still affect performance.
- **Window size**: The default 60 days is a reasonable starting point; you may experiment with 30, 90, or 120 days.
- **Feature engineering**: Consider adding technical indicators (moving averages, RSI, MACD) to improve predictive power.
- **Overfitting**: Despite heavy regularisation, financial time series are notoriously noisy. Always validate on outâ€‘ofâ€‘time data.
- **GPU memory**: If you encounter outâ€‘ofâ€‘memory errors, reduce `BATCH_SIZE` or the number of filters in the convolutional layers.

---

## ğŸ“š References

- TensorFlow/Keras: [https://www.tensorflow.org/](https://www.tensorflow.org/)
- scikitâ€‘learn: [https://scikit-learn.org/](https://scikit-learn.org/)
- Pandas: [https://pandas.pydata.org/](https://pandas.pydata.org/)
- Matplotlib: [https://matplotlib.org/](https://matplotlib.org/)

For questions or contributions, please open an issue in the project repository.

---
